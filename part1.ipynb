{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import qalsadi.lemmatizer\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de Donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...\n",
       "1  وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...\n",
       "2  يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...\n",
       "3  شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...\n",
       "4  قال وزير الخارجية الأميركي أنتوني بلينكن -اليو..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Arabic text\n",
    "df = pd.read_csv('textes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score data with topic \"Gaza\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...</td>\n",
       "      <td>3.117674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...</td>\n",
       "      <td>2.003642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...</td>\n",
       "      <td>2.882009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>أثار فيديو سقوط أحد الفلسطينيين أسفل عجلات شاح...</td>\n",
       "      <td>3.038653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     score\n",
       "0  أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...  3.117674\n",
       "1  وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...  0.000000\n",
       "2  يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...  0.000000\n",
       "3  شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...  2.003642\n",
       "4  قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...  2.882009\n",
       "5  أثار فيديو سقوط أحد الفلسطينيين أسفل عجلات شاح...  3.038653"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text for text in df['text']]\n",
    "\n",
    "query = \" \".join([\"غزة حماس\", \"إسرائيل\", \"الاحتلال\", \"الصراع\", \"القتلى\", \"الجرحى\", \"الهدنة\", \"المقاومة\", \"قصف\", \"ضربات\", \"مقاتلين\"])\n",
    "\n",
    "\n",
    "# Vectorize the Texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
    "\n",
    "# Calculate Relevance Scores\n",
    "query_vector = tfidf_matrix[-1]\n",
    "text_vectors = tfidf_matrix[:-1]\n",
    "\n",
    "# Compute cosine similarity between the query and each text\n",
    "cosine_similarities = cosine_similarity(query_vector, text_vectors).flatten()\n",
    "\n",
    "# Normalize Scores\n",
    "normalized_scores = (cosine_similarities - cosine_similarities.min()) / (cosine_similarities.max() - cosine_similarities.min()) * 10\n",
    "\n",
    "df['score']  = normalized_scores\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-traitement des donnees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(texte):\n",
    "\n",
    "    # Supprimer les caractères spéciaux\n",
    "    texte = re.sub(r'[^\\w\\s]', '', texte)\n",
    "\n",
    "    # Supprimer les retours à la ligne\n",
    "    texte = re.sub(r'\\n', ' ', texte)\n",
    "\n",
    "    # Supprimer les espaces doubles\n",
    "    texte = re.sub(r'\\s+', ' ', texte)\n",
    "\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    return lemmer.lemmatize_text(text, return_pos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWord(text):\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    filtered_tokens = [word for word in text if word not in arabic_stopwords]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = preprocessing(text).split()\n",
    "    text = stopWord(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...</td>\n",
       "      <td>3.117674</td>\n",
       "      <td>[أكدت, صحيفة, واشنطن, بوست, اليوم, الثلاثاء, إ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[وصل, اليوم, الثلاثاء, مدينة, قم, المقدسة, الإ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[يؤدي, التعب, والإرهاق, الناتج, الرحلات, المتت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...</td>\n",
       "      <td>2.003642</td>\n",
       "      <td>[شاعر, ناهض, النظام, الناصري, ولم, يهادن, نظام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...</td>\n",
       "      <td>2.882009</td>\n",
       "      <td>[قال, وزير, الخارجية, الأميركي, أنتوني, بلينكن...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     score  \\\n",
       "0  أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...  3.117674   \n",
       "1  وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...  0.000000   \n",
       "2  يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...  0.000000   \n",
       "3  شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...  2.003642   \n",
       "4  قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...  2.882009   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  [أكدت, صحيفة, واشنطن, بوست, اليوم, الثلاثاء, إ...  \n",
       "1  [وصل, اليوم, الثلاثاء, مدينة, قم, المقدسة, الإ...  \n",
       "2  [يؤدي, التعب, والإرهاق, الناتج, الرحلات, المتت...  \n",
       "3  [شاعر, ناهض, النظام, الناصري, ولم, يهادن, نظام...  \n",
       "4  [قال, وزير, الخارجية, الأميركي, أنتوني, بلينكن...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data embendded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.ar.300.bin\\cc.ar.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    word_vectors = [ft.get_word_vector(word) for word in sentence]\n",
    "    # Calculer la moyenne des vecteurs de mots\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_vectorized'] = df['text_cleaned'].apply(vectorize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...</td>\n",
       "      <td>3.117674</td>\n",
       "      <td>[أكدت, صحيفة, واشنطن, بوست, اليوم, الثلاثاء, إ...</td>\n",
       "      <td>[0.012120141, 0.041061074, -0.016316786, 0.038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[وصل, اليوم, الثلاثاء, مدينة, قم, المقدسة, الإ...</td>\n",
       "      <td>[-0.006007621, -0.0028243384, -0.0027309244, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[يؤدي, التعب, والإرهاق, الناتج, الرحلات, المتت...</td>\n",
       "      <td>[-0.02321628, 0.0063483943, -0.011471126, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...</td>\n",
       "      <td>2.003642</td>\n",
       "      <td>[شاعر, ناهض, النظام, الناصري, ولم, يهادن, نظام...</td>\n",
       "      <td>[-0.0022376042, 0.028561369, 0.012915291, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...</td>\n",
       "      <td>2.882009</td>\n",
       "      <td>[قال, وزير, الخارجية, الأميركي, أنتوني, بلينكن...</td>\n",
       "      <td>[0.022290997, 0.023376264, -0.02258841, 0.0464...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     score  \\\n",
       "0  أكدت صحيفة واشنطن بوست اليوم الثلاثاء أن إسرائ...  3.117674   \n",
       "1  وصل اليوم الثلاثاء، إلى مدينة قم المقدسة لدى ا...  0.000000   \n",
       "2  يؤدي التعب والإرهاق الناتج عن الرحلات المتتالي...  0.000000   \n",
       "3  شاعر ناهض النظام الناصري، ولم يهادن نظام الساد...  2.003642   \n",
       "4  قال وزير الخارجية الأميركي أنتوني بلينكن -اليو...  2.882009   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  [أكدت, صحيفة, واشنطن, بوست, اليوم, الثلاثاء, إ...   \n",
       "1  [وصل, اليوم, الثلاثاء, مدينة, قم, المقدسة, الإ...   \n",
       "2  [يؤدي, التعب, والإرهاق, الناتج, الرحلات, المتت...   \n",
       "3  [شاعر, ناهض, النظام, الناصري, ولم, يهادن, نظام...   \n",
       "4  [قال, وزير, الخارجية, الأميركي, أنتوني, بلينكن...   \n",
       "\n",
       "                                     text_vectorized  \n",
       "0  [0.012120141, 0.041061074, -0.016316786, 0.038...  \n",
       "1  [-0.006007621, -0.0028243384, -0.0027309244, 0...  \n",
       "2  [-0.02321628, 0.0063483943, -0.011471126, 0.03...  \n",
       "3  [-0.0022376042, 0.028561369, 0.012915291, 0.04...  \n",
       "4  [0.022290997, 0.023376264, -0.02258841, 0.0464...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('textes_vectorization.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['score'].values\n",
    "X = np.array(df['text_vectorized'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, scores):\n",
    "        self.texts = [torch.tensor(text, dtype=torch.float32).unsqueeze(0) for text in texts]\n",
    "        self.scores = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.scores[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()  # Si vous faites une régression, sinon utilisez CrossEntropyLoss pour la classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "def train(train_loader,test_loader,model,num_epochs,optimizer,criterion):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for texts, scores in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs.squeeze(), scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for texts, scores in test_loader:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs.squeeze(), scores)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 18.7548\n",
      "Epoch [2/100], Loss: 1.0405\n",
      "Epoch [3/100], Loss: 1.4478\n",
      "Epoch [4/100], Loss: 5.3831\n",
      "Epoch [5/100], Loss: 1.3429\n",
      "Epoch [6/100], Loss: 0.3651\n",
      "Epoch [7/100], Loss: 0.3930\n",
      "Epoch [8/100], Loss: 0.4668\n",
      "Epoch [9/100], Loss: 0.6961\n",
      "Epoch [10/100], Loss: 0.4449\n",
      "Epoch [11/100], Loss: 16.6715\n",
      "Epoch [12/100], Loss: 1.1875\n",
      "Epoch [13/100], Loss: 0.4123\n",
      "Epoch [14/100], Loss: 0.6041\n",
      "Epoch [15/100], Loss: 4.3253\n",
      "Epoch [16/100], Loss: 4.3701\n",
      "Epoch [17/100], Loss: 0.7145\n",
      "Epoch [18/100], Loss: 0.5718\n",
      "Epoch [19/100], Loss: 1.1308\n",
      "Epoch [20/100], Loss: 4.7605\n",
      "Epoch [21/100], Loss: 1.7571\n",
      "Epoch [22/100], Loss: 0.4038\n",
      "Epoch [23/100], Loss: 0.6347\n",
      "Epoch [24/100], Loss: 1.0951\n",
      "Epoch [25/100], Loss: 4.1749\n",
      "Epoch [26/100], Loss: 0.4867\n",
      "Epoch [27/100], Loss: 0.3954\n",
      "Epoch [28/100], Loss: 0.4173\n",
      "Epoch [29/100], Loss: 0.2913\n",
      "Epoch [30/100], Loss: 0.2755\n",
      "Epoch [31/100], Loss: 0.3656\n",
      "Epoch [32/100], Loss: 0.4857\n",
      "Epoch [33/100], Loss: 0.4929\n",
      "Epoch [34/100], Loss: 0.4178\n",
      "Epoch [35/100], Loss: 3.0653\n",
      "Epoch [36/100], Loss: 0.2046\n",
      "Epoch [37/100], Loss: 2.9267\n",
      "Epoch [38/100], Loss: 1.2421\n",
      "Epoch [39/100], Loss: 0.7239\n",
      "Epoch [40/100], Loss: 0.7094\n",
      "Epoch [41/100], Loss: 0.2824\n",
      "Epoch [42/100], Loss: 2.1676\n",
      "Epoch [43/100], Loss: 0.7204\n",
      "Epoch [44/100], Loss: 0.8628\n",
      "Epoch [45/100], Loss: 0.1781\n",
      "Epoch [46/100], Loss: 1.7352\n",
      "Epoch [47/100], Loss: 1.5218\n",
      "Epoch [48/100], Loss: 0.4447\n",
      "Epoch [49/100], Loss: 1.0663\n",
      "Epoch [50/100], Loss: 0.4087\n",
      "Epoch [51/100], Loss: 0.6954\n",
      "Epoch [52/100], Loss: 10.0572\n",
      "Epoch [53/100], Loss: 0.5244\n",
      "Epoch [54/100], Loss: 0.1499\n",
      "Epoch [55/100], Loss: 1.1560\n",
      "Epoch [56/100], Loss: 0.6246\n",
      "Epoch [57/100], Loss: 0.3030\n",
      "Epoch [58/100], Loss: 0.4282\n",
      "Epoch [59/100], Loss: 0.2442\n",
      "Epoch [60/100], Loss: 0.7988\n",
      "Epoch [61/100], Loss: 0.4267\n",
      "Epoch [62/100], Loss: 8.5870\n",
      "Epoch [63/100], Loss: 0.1396\n",
      "Epoch [64/100], Loss: 0.7295\n",
      "Epoch [65/100], Loss: 0.1918\n",
      "Epoch [66/100], Loss: 0.7430\n",
      "Epoch [67/100], Loss: 0.1986\n",
      "Epoch [68/100], Loss: 0.1671\n",
      "Epoch [69/100], Loss: 0.4005\n",
      "Epoch [70/100], Loss: 0.1018\n",
      "Epoch [71/100], Loss: 0.3902\n",
      "Epoch [72/100], Loss: 0.3257\n",
      "Epoch [73/100], Loss: 0.1389\n",
      "Epoch [74/100], Loss: 0.4488\n",
      "Epoch [75/100], Loss: 0.1693\n",
      "Epoch [76/100], Loss: 0.2680\n",
      "Epoch [77/100], Loss: 0.8756\n",
      "Epoch [78/100], Loss: 0.5980\n",
      "Epoch [79/100], Loss: 0.2374\n",
      "Epoch [80/100], Loss: 0.2607\n",
      "Epoch [81/100], Loss: 0.5447\n",
      "Epoch [82/100], Loss: 0.2147\n",
      "Epoch [83/100], Loss: 6.2643\n",
      "Epoch [84/100], Loss: 0.3073\n",
      "Epoch [85/100], Loss: 0.3156\n",
      "Epoch [86/100], Loss: 0.0685\n",
      "Epoch [87/100], Loss: 0.1195\n",
      "Epoch [88/100], Loss: 0.0724\n",
      "Epoch [89/100], Loss: 0.0493\n",
      "Epoch [90/100], Loss: 0.4496\n",
      "Epoch [91/100], Loss: 0.4689\n",
      "Epoch [92/100], Loss: 0.0524\n",
      "Epoch [93/100], Loss: 5.3302\n",
      "Epoch [94/100], Loss: 0.2362\n",
      "Epoch [95/100], Loss: 0.2560\n",
      "Epoch [96/100], Loss: 0.2186\n",
      "Epoch [97/100], Loss: 0.1603\n",
      "Epoch [98/100], Loss: 0.3107\n",
      "Epoch [99/100], Loss: 0.3341\n",
      "Epoch [100/100], Loss: 0.3259\n",
      "Test Loss: 0.7233\n"
     ]
    }
   ],
   "source": [
    "outputs = train(train_loader,test_loader,model,100,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7233337499079836"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(outputs,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSProp', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - loss: 3.6723 - val_loss: 1.3951\n",
      "Epoch 2/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.2950 - val_loss: 1.3188\n",
      "Epoch 3/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9765 - val_loss: 1.2534\n",
      "Epoch 4/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.5816 - val_loss: 1.2122\n",
      "Epoch 5/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8352 - val_loss: 1.1635\n",
      "Epoch 6/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2330 - val_loss: 1.1414\n",
      "Epoch 7/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1663 - val_loss: 1.1000\n",
      "Epoch 8/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.2346 - val_loss: 1.0749\n",
      "Epoch 9/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0942 - val_loss: 1.0479\n",
      "Epoch 10/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.3146 - val_loss: 1.0376\n",
      "Epoch 11/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.6195 - val_loss: 1.0248\n",
      "Epoch 12/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3947 - val_loss: 1.0172\n",
      "Epoch 13/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2478 - val_loss: 1.0090\n",
      "Epoch 14/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.9156 - val_loss: 1.0031\n",
      "Epoch 15/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.4252 - val_loss: 0.9966\n",
      "Epoch 16/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4992 - val_loss: 0.9903\n",
      "Epoch 17/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.6724 - val_loss: 0.9851\n",
      "Epoch 18/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 2.2570 - val_loss: 0.9838\n",
      "Epoch 19/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3736 - val_loss: 0.9815\n",
      "Epoch 20/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.0337 - val_loss: 0.9721\n",
      "Epoch 21/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.7660 - val_loss: 0.9648\n",
      "Epoch 22/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.2086 - val_loss: 0.9579\n",
      "Epoch 23/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1535 - val_loss: 0.9478\n",
      "Epoch 24/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.7713 - val_loss: 0.9393\n",
      "Epoch 25/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.7543 - val_loss: 0.9400\n",
      "Epoch 26/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.8374 - val_loss: 0.9480\n",
      "Epoch 27/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.0655 - val_loss: 0.9398\n",
      "Epoch 28/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.0275 - val_loss: 0.9301\n",
      "Epoch 29/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5875 - val_loss: 0.9208\n",
      "Epoch 30/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.6697 - val_loss: 0.9049\n",
      "Epoch 31/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.1068 - val_loss: 0.9006\n",
      "Epoch 32/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5341 - val_loss: 0.8932\n",
      "Epoch 33/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.4211 - val_loss: 0.8887\n",
      "Epoch 34/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.9289 - val_loss: 0.8973\n",
      "Epoch 35/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.8973 - val_loss: 0.8954\n",
      "Epoch 36/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.4070 - val_loss: 0.8963\n",
      "Epoch 37/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.5088 - val_loss: 0.8779\n",
      "Epoch 38/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6493 - val_loss: 0.8917\n",
      "Epoch 39/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.9249 - val_loss: 0.8649\n",
      "Epoch 40/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.1766 - val_loss: 0.8472\n",
      "Epoch 41/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.1617 - val_loss: 0.8302\n",
      "Epoch 42/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6742 - val_loss: 0.8167\n",
      "Epoch 43/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.6410 - val_loss: 0.8219\n",
      "Epoch 44/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.0769 - val_loss: 0.8350\n",
      "Epoch 45/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.6750 - val_loss: 0.8121\n",
      "Epoch 46/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0978 - val_loss: 0.7912\n",
      "Epoch 47/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.6690 - val_loss: 0.7827\n",
      "Epoch 48/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6170 - val_loss: 0.7740\n",
      "Epoch 49/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.5789 - val_loss: 0.7810\n",
      "Epoch 50/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.0017 - val_loss: 0.7651\n",
      "Epoch 51/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.9689 - val_loss: 0.7823\n",
      "Epoch 52/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0058 - val_loss: 0.7580\n",
      "Epoch 53/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3749 - val_loss: 0.7685\n",
      "Epoch 54/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.9769 - val_loss: 0.7357\n",
      "Epoch 55/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.4241 - val_loss: 0.7281\n",
      "Epoch 56/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.5283 - val_loss: 0.7121\n",
      "Epoch 57/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.3953 - val_loss: 0.6927\n",
      "Epoch 58/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.2470 - val_loss: 0.7125\n",
      "Epoch 59/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.3888 - val_loss: 0.6983\n",
      "Epoch 60/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.2816 - val_loss: 0.6810\n",
      "Epoch 61/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.7222 - val_loss: 0.6849\n",
      "Epoch 62/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.7367 - val_loss: 0.6804\n",
      "Epoch 63/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1886 - val_loss: 0.6953\n",
      "Epoch 64/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6015 - val_loss: 0.6875\n",
      "Epoch 65/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.6980 - val_loss: 0.6778\n",
      "Epoch 66/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.2870 - val_loss: 0.6545\n",
      "Epoch 67/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.0270 - val_loss: 0.6809\n",
      "Epoch 68/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.5443 - val_loss: 0.6585\n",
      "Epoch 69/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1381 - val_loss: 0.6400\n",
      "Epoch 70/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1765 - val_loss: 0.6340\n",
      "Epoch 71/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1509 - val_loss: 0.6363\n",
      "Epoch 72/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1539 - val_loss: 0.6264\n",
      "Epoch 73/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.4309 - val_loss: 0.6147\n",
      "Epoch 74/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.1758 - val_loss: 0.6062\n",
      "Epoch 75/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.0543 - val_loss: 0.6111\n",
      "Epoch 76/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.4470 - val_loss: 0.6058\n",
      "Epoch 77/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.0710 - val_loss: 0.6059\n",
      "Epoch 78/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3722 - val_loss: 0.5940\n",
      "Epoch 79/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.4291 - val_loss: 0.5985\n",
      "Epoch 80/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9692 - val_loss: 0.5932\n",
      "Epoch 81/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3384 - val_loss: 0.5869\n",
      "Epoch 82/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3128 - val_loss: 0.5828\n",
      "Epoch 83/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.9599 - val_loss: 0.5877\n",
      "Epoch 84/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.9414 - val_loss: 0.5863\n",
      "Epoch 85/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7722 - val_loss: 0.6130\n",
      "Epoch 86/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.2138 - val_loss: 0.6329\n",
      "Epoch 87/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8805 - val_loss: 0.6104\n",
      "Epoch 88/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1720 - val_loss: 0.6033\n",
      "Epoch 89/89\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.1950 - val_loss: 0.6205\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=89, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6205\n",
      "Test loss: 0.6204981207847595\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - loss: 3.0113 - val_loss: 1.3397\n",
      "Epoch 2/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7316 - val_loss: 1.2593\n",
      "Epoch 3/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.7614 - val_loss: 1.1944\n",
      "Epoch 4/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6925 - val_loss: 1.1415\n",
      "Epoch 5/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3915 - val_loss: 1.0964\n",
      "Epoch 6/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5983 - val_loss: 1.0643\n",
      "Epoch 7/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3378 - val_loss: 1.0444\n",
      "Epoch 8/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.2658 - val_loss: 1.0316\n",
      "Epoch 9/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.9834 - val_loss: 1.0229\n",
      "Epoch 10/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5641 - val_loss: 1.0165\n",
      "Epoch 11/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.7970 - val_loss: 1.0099\n",
      "Epoch 12/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.0110 - val_loss: 1.0038\n",
      "Epoch 13/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.2421 - val_loss: 0.9955\n",
      "Epoch 14/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2436 - val_loss: 0.9868\n",
      "Epoch 15/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8432 - val_loss: 0.9782\n",
      "Epoch 16/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8253 - val_loss: 0.9689\n",
      "Epoch 17/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.1963 - val_loss: 0.9599\n",
      "Epoch 18/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.7206 - val_loss: 0.9516\n",
      "Epoch 19/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.6883 - val_loss: 0.9428\n",
      "Epoch 20/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.0813 - val_loss: 0.9321\n",
      "Epoch 21/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2150 - val_loss: 0.9218\n",
      "Epoch 22/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7265 - val_loss: 0.9086\n",
      "Epoch 23/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.9053 - val_loss: 0.8938\n",
      "Epoch 24/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9596 - val_loss: 0.8786\n",
      "Epoch 25/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5114 - val_loss: 0.8650\n",
      "Epoch 26/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8319 - val_loss: 0.8499\n",
      "Epoch 27/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6138 - val_loss: 0.8368\n",
      "Epoch 28/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8427 - val_loss: 0.8311\n",
      "Epoch 29/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7662 - val_loss: 0.8248\n",
      "Epoch 30/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4155 - val_loss: 0.8185\n",
      "Epoch 31/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.7034 - val_loss: 0.8061\n",
      "Epoch 32/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2732 - val_loss: 0.7988\n",
      "Epoch 33/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2366 - val_loss: 0.7898\n",
      "Epoch 34/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6303 - val_loss: 0.7767\n",
      "Epoch 35/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.9745 - val_loss: 0.7644\n",
      "Epoch 36/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0252 - val_loss: 0.7472\n",
      "Epoch 37/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4936 - val_loss: 0.7259\n",
      "Epoch 38/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.8647 - val_loss: 0.7051\n",
      "Epoch 39/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1923 - val_loss: 0.6908\n",
      "Epoch 40/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4356 - val_loss: 0.6847\n",
      "Epoch 41/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1223 - val_loss: 0.6822\n",
      "Epoch 42/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3404 - val_loss: 0.6931\n",
      "Epoch 43/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.7060 - val_loss: 0.6982\n",
      "Epoch 44/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1963 - val_loss: 0.7006\n",
      "Epoch 45/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.5832 - val_loss: 0.7055\n",
      "Epoch 46/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.2391 - val_loss: 0.6960\n",
      "Epoch 47/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.1793 - val_loss: 0.6787\n",
      "Epoch 48/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4838 - val_loss: 0.6587\n",
      "Epoch 49/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0005 - val_loss: 0.6481\n",
      "Epoch 50/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8901 - val_loss: 0.6579\n",
      "Epoch 51/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3847 - val_loss: 0.6868\n",
      "Epoch 52/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3395 - val_loss: 0.7059\n",
      "Epoch 53/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.3425 - val_loss: 0.6990\n",
      "Epoch 54/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.2945 - val_loss: 0.6752\n",
      "Epoch 55/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.9661 - val_loss: 0.6446\n",
      "Epoch 56/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7524 - val_loss: 0.6281\n",
      "Epoch 57/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8963 - val_loss: 0.6329\n",
      "Epoch 58/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8962 - val_loss: 0.6341\n",
      "Epoch 59/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1813 - val_loss: 0.6336\n",
      "Epoch 60/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8248 - val_loss: 0.6289\n",
      "Epoch 61/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0885 - val_loss: 0.6293\n",
      "Epoch 62/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6676 - val_loss: 0.6352\n",
      "Epoch 63/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8420 - val_loss: 0.6536\n",
      "Epoch 64/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0990 - val_loss: 0.6730\n",
      "Epoch 65/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0197 - val_loss: 0.6852\n",
      "Epoch 66/66\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9905 - val_loss: 0.6878\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=66, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.6878\n",
      "Test loss: 0.6877862215042114\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
